{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('TSLA.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "first_index = data['Firm'].first_valid_index()\n",
    "\n",
    "# Drop everything before that index (if it exists)\n",
    "if first_index is not None:\n",
    "    data = data.loc[first_index:].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_event_dataframes(data):\n",
    "    event_dfs = {}\n",
    "    \n",
    "    # Define a mapping of ToGrade values to event phrases\n",
    "    action_map = {\n",
    "        'Buy': 'suggests buy',\n",
    "        'Sell': 'suggests sell',\n",
    "        'Outperform': 'suggests outperform',\n",
    "        'Underperform': 'suggests underperform',\n",
    "        'Overweight': 'suggests overweight',\n",
    "        'Underweight': 'suggests underweight',\n",
    "        'Neutral': 'suggests hold',\n",
    "        'Higher': 'suggests higher',\n",
    "        'Lower': 'suggests lower',\n",
    "        # Add any other mappings you need here\n",
    "    }\n",
    "\n",
    "    # Iterate through each unique firm\n",
    "    for firm in data['Firm'].unique():\n",
    "        # Filter rows for the current firm\n",
    "        firm_data = data[data['Firm'] == firm]\n",
    "        \n",
    "        # Iterate through the unique suggestions in ToGrade\n",
    "        for to_grade in firm_data['ToGrade'].unique():\n",
    "            # Convert to_grade to a string to avoid AttributeError\n",
    "            to_grade_str = str(to_grade)\n",
    "\n",
    "            # Map the ToGrade to an event phrase\n",
    "            action_phrase = action_map.get(to_grade_str, f'suggests {to_grade_str.lower()}')\n",
    "\n",
    "            # Create a temporary DataFrame for the current event\n",
    "            temp_event_df = pd.DataFrame({\n",
    "                'ds': pd.to_datetime(firm_data[firm_data['ToGrade'] == to_grade]['Date']),\n",
    "                'event': f\"{firm} {action_phrase}\"  # Combine firm name and action\n",
    "            })\n",
    "            \n",
    "            # Merge the event DataFrame into event_dfs\n",
    "            if not temp_event_df.empty:\n",
    "                # Group by 'event' and aggregate dates as formatted strings\n",
    "                grouped_df = temp_event_df.groupby('event')['ds'].agg(lambda x: list(x.dt.strftime('%Y-%m-%d'))).reset_index()\n",
    "                event_dfs[(firm, to_grade_str)] = grouped_df  # Store by firm and ToGrade\n",
    "\n",
    "    return event_dfs\n",
    "\n",
    "# Example usage\n",
    "event_dfs = create_event_dataframes(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changes name based on firm\n",
    "# Load data into a DataFrame\n",
    "# Group by Date and aggregate\n",
    "merged_df = data.groupby('Date').agg({\n",
    "    'Open': 'first',               # First value of Open\n",
    "    'High': 'max',                 # Max value of High\n",
    "    'Low': 'min',                  # Min value of Low\n",
    "    'Close': 'last',               # Last value of Close\n",
    "    'Volume': 'sum',               # Total Volume\n",
    "    'Dividends': 'first',          # First value of Dividends\n",
    "    'Stock Splits': 'first',        # First value of Stock Splits\n",
    "    'Reported EPS': 'first',\n",
    "    'EPS Estimate': 'first',\n",
    "    'Surprise(%)': 'first',\n",
    "}).reset_index()\n",
    "\n",
    "# Create unique columns for Firm, ToGrade, FromGrade, and Action\n",
    "unique_firms = data['Firm'].unique()\n",
    "\n",
    "for firm in unique_firms:\n",
    "    firm_data = data[data['Firm'] == firm]\n",
    "    merged_df[firm] = firm_data.groupby('Date')['Firm'].transform(lambda x: x.iloc[0] if not x.empty else None)\n",
    "    merged_df[f'ToGrade {firm}'] = firm_data.groupby('Date')['ToGrade'].transform(lambda x: x.iloc[0] if not x.empty else None)\n",
    "    merged_df[f'FromGrade {firm} '] = firm_data.groupby('Date')['FromGrade'].transform(lambda x: x.iloc[0] if not x.empty else None)\n",
    "    merged_df[f'Action {firm}'] = firm_data.groupby('Date')['Action'].transform(lambda x: x.iloc[0] if not x.empty else None)\n",
    "    merged_df = merged_df.drop(firm, axis=1) #without this it just appends firm\n",
    "\n",
    "\n",
    "# Display the merged DataFrame\n",
    "\n",
    "data = merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Reported EPS'] = data['Reported EPS'].fillna(method='ffill') #this is ffill cause we knew it but don't know what the future will be\n",
    "data['EPS Estimate'] = data['EPS Estimate'].fillna(method='bfill') #This is bfill cause we know the estimate ahead of time\n",
    "data['Surprise(%)'] = data['Surprise(%)'].fillna(method='ffill')\n",
    "data['Surprise(%)'] = data['Surprise(%)'].fillna(0)\n",
    "data['EPS Estimate'] = data['EPS Estimate'].fillna(method='ffill') #For future prediction\n",
    "data['Reported EPS'] = data['Reported EPS'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (data['Dividends'] == 0).all():\n",
    "        print(\"Dropping Dividends\")\n",
    "        data = data.drop(columns=['Dividends'])\n",
    "if (data['Stock Splits'] == 0).all():\n",
    "        print(\"Dropping StockSplits\")\n",
    "        data = data.drop(columns=['Stock Splits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralprophet import NeuralProphet\n",
    "#from prophet import Prophet\n",
    "\n",
    "from neuralprophet import set_random_seed\n",
    "\n",
    "set_random_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epochs and learning_rate, and potentially increase the batch_size\n",
    "# Initialize the Prophet model\n",
    "#Create multiple models\n",
    "#Needed to individually create model objects or it errors!\n",
    "Openmodel = NeuralProphet(\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    daily_seasonality=False,\n",
    "    epochs=7000,\n",
    "\n",
    "    n_changepoints=20,\n",
    "    trend_global_local=\"local\",\n",
    "    #growth=\"logistic\", #currently unsupported feature\n",
    "    seasonality_mode=\"multiplicative\",\n",
    "\n",
    "    batch_size=16,\n",
    "\n",
    "    learning_rate=0.0001,\n",
    "    #ar_layers=200\n",
    "\n",
    "    #trainer_config={\"accelerator\": \"mps\"}\n",
    ")\n",
    "Closemodel = NeuralProphet(\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    daily_seasonality=False,\n",
    "    epochs=7000,\n",
    "\n",
    "    n_changepoints=20,\n",
    "    trend_global_local=\"local\",\n",
    "\n",
    "    seasonality_mode=\"multiplicative\",\n",
    "\n",
    "    batch_size=16,\n",
    "\n",
    "    learning_rate=0.0001,\n",
    "    #ar_layers=200\n",
    "\n",
    "    #trainer_config={\"accelerator\": \"mps\"}\n",
    ")\n",
    "Volumemodel = NeuralProphet(\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    daily_seasonality=False,\n",
    "    epochs=7000,\n",
    "\n",
    "    n_changepoints=20,\n",
    "    trend_global_local=\"local\",\n",
    "    #growth=\"logistic\", #currently unsupported feature\n",
    "    seasonality_mode=\"multiplicative\",\n",
    "\n",
    "    batch_size=16,\n",
    "    #learning_rate=0.00005,\n",
    "    learning_rate=0.0001,\n",
    "    #ar_layers=7\n",
    "\n",
    "    #trainer_config={\"accelerator\": \"mps\"}\n",
    ")\n",
    "\n",
    "Reportedepsmodel = NeuralProphet(\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    daily_seasonality=False,\n",
    "    epochs=7000,\n",
    "\n",
    "    n_changepoints=20,\n",
    "    trend_global_local=\"local\",\n",
    "    #growth=\"logistic\", #currently unsupported feature\n",
    "    seasonality_mode=\"multiplicative\",\n",
    "\n",
    "    batch_size=16,\n",
    "    learning_rate=0.0001,\n",
    "    #ar_layers=7\n",
    "\n",
    "    #trainer_config={\"accelerator\": \"mps\"}\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for col in data.columns:\n",
    "    if col.startswith('ToGrade') or col.startswith('FromGrade') or col.startswith('Action'):\n",
    "        data = data.drop(col, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename(columns={'Date': 'ds'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = data.drop(\"High\", axis=1) #dropped to make room for eps\n",
    "data = data.drop(\"Low\", axis=1) #dropped to make room for eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Openmodeldata = Closemodeldata = Volumedata = Reportedepsdata = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Openmodeldata['y'] = data['Open']\n",
    "Openmodeldata = Openmodeldata.drop(\"Open\", axis=1)\n",
    "Closemodeldata['y'] = data['Close']\n",
    "Closemodeldata = Closemodeldata.drop(\"Close\", axis=1)\n",
    "Volumedata['y'] = data['Volume']\n",
    "Volumedata = Volumedata.drop(\"Volume\", axis=1)\n",
    "Reportedepsdata['y'] = data['Reported EPS']\n",
    "Reportedepsdata = Reportedepsdata.drop(\"Reported EPS\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Did it this way because it uses functions that modify the neuralprophet object. I would have prefered just to modify data then copy\n",
    "def add_events_to_model(event_dfs, model):\n",
    "    for (firm, to_grade), event_df in event_dfs.items():\n",
    "        for _, row in event_df.iterrows():\n",
    "            event_name = row['event']\n",
    "            event_dates = row['ds']  # This will be a list of dates\n",
    "\n",
    "            # Print the event details for confirmation\n",
    "            print(f\"Adding event: {event_name} on dates: {event_dates}\")\n",
    "\n",
    "            # Add the event name to the model\n",
    "            try:\n",
    "                model = model.add_events(event_name)\n",
    "               \n",
    "               # df_all = m.create_df_with_events(df, event_df)\n",
    "                \n",
    "                print(f\"Successfully added event: {event_name}\")\n",
    "            except ValueError as e:\n",
    "                print(f\"Error adding event {event_name}: {e}\")\n",
    "\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "Openmodel = add_events_to_model(event_dfs, Openmodel)\n",
    "Closemodel = add_events_to_model(event_dfs, Closemodel)\n",
    "Volumemodel = add_events_to_model(event_dfs, Volumemodel)\n",
    "Reportedepsmodel = add_events_to_model(event_dfs, Reportedepsmodel)\n",
    "#Add all the stuff to the data\n",
    "for (firm, to_grade), event_df in event_dfs.items():\n",
    "    Openmodeldata = Openmodel.create_df_with_events(Openmodeldata, event_df)\n",
    "    Closemodeldata = Closemodel.create_df_with_events(Closemodeldata, event_df)\n",
    "    Volumedata = Volumemodel.create_df_with_events(Volumedata, event_df)\n",
    "    Reportedepsdata = Reportedepsmodel.create_df_with_events(Reportedepsdata, event_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'Firm', 'ToGrade', 'FromGrade', 'Action'\n",
    "def addregressorstotal(m, dataform):\n",
    "    m = m.add_country_holidays(\"US\") #add holidays to our trends\n",
    "    if \"Close\" in dataform.columns:\n",
    "        m = m.add_lagged_regressor(\"Close\", n_lags=7)\n",
    "    if \"Volume\" in dataform.columns:\n",
    "        m = m.add_lagged_regressor(\"Volume\", n_lags=7)\n",
    "    if \"Open\" in dataform.columns:\n",
    "        m = m.add_lagged_regressor(\"Open\", n_lags=7)\n",
    "    if \"Reported EPS\" in dataform.columns:\n",
    "        m = m.add_lagged_regressor(\"Reported EPS\", n_lags=1)\n",
    "    m = m.add_lagged_regressor(\"Surprise(%)\", n_lags=1)\n",
    "    m = m.add_future_regressor(\"EPS Estimate\", mode='additive') #Perhaps make \"multiplicative\" depending\n",
    "    if ('Dividends' in dataform.columns):\n",
    "         m = m.add_future_regressor(\"Dividends\", mode='additive') \n",
    "    if ('Stock Splits' in dataform.columns):\n",
    "        m = m.add_future_regressor(\"Stock Splits\", mode='additive')\n",
    "addregressorstotal(Openmodel,Openmodeldata)\n",
    "addregressorstotal(Closemodel,Closemodeldata)\n",
    "addregressorstotal(Volumemodel,Volumedata)\n",
    "addregressorstotal(Reportedepsmodel,Reportedepsdata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricsOpen = Openmodel.fit(Openmodeldata, early_stopping = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricsClose = Closemodel.fit(Closemodeldata, early_stopping = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricsVolume = Volumemodel.fit(Volumedata, early_stopping = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricsReportedeps = Reportedepsmodel.fit(Reportedepsdata, early_stopping = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecastOpen = Openmodel.predict(Openmodeldata)\n",
    "forecastClose = Closemodel.predict(Closemodeldata)\n",
    "forecastVolume = Volumemodel.predict(Volumedata)\n",
    "forecastReportedeps = Reportedepsmodel.predict(Reportedepsdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Openmodel.plot(forecastOpen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Closemodel.plot(forecastClose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Volumemodel.plot(forecastVolume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reportedepsmodel.plot(forecastReportedeps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_dfOpen = Openmodeldata.copy()\n",
    "future_dfClose = Closemodeldata.copy()\n",
    "future_dfVolume = Volumedata.copy()\n",
    "future_dfReportedeps = Reportedepsdata.copy()\n",
    "Cleanedfuture=pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(future_dfOpen['Surprise(%)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.tseries.offsets import BDay\n",
    "\n",
    "# Assuming `data` is your training DataFrame\n",
    "# Step 1: Clone the existing DataFrame\n",
    "#forecast = pd.DataFrame()\n",
    "def predictFuture(data, model):\n",
    "    #notes predict does 1 day behind cause it can't go forward so we have to move everything forward again\n",
    "    future_df = data.copy()\n",
    "\n",
    "# Step 2: Generate future business days\n",
    "    last_date = future_df['ds'].max()\n",
    "    future_periods = 1  # Specify how many future days you want\n",
    "    future_dates = pd.date_range(start=last_date + BDay(1), periods=future_periods, freq=BDay())\n",
    "    #future_dates = future_dates.date\n",
    "\n",
    "# Step 3: Create a new DataFrame for future periods\n",
    "    last_eps_estimate = data['EPS Estimate'].tail(1).values[0]  # Get the last EPS Estimate value\n",
    "\n",
    "# Initialize future entries with the last values from the original data\n",
    "    future_entries = {\n",
    "        'ds': future_dates,\n",
    "        'EPS Estimate': [last_eps_estimate] * future_periods,  # Use the last value\n",
    "    }\n",
    "    #future_entries['ds'] = future_entries['ds'].strftime('%Y/%m/%d')\n",
    "    #print(\"HEEERE\")\n",
    "    #print(future_entries['ds'])\n",
    "\n",
    "# Fill in other columns with the last available value\n",
    "    for column in data.columns:\n",
    "        if column != 'ds':  # Skip the 'ds' column since we are generating new dates\n",
    "            if column not in ['Dividends', 'Stock Splits']:  # Handle these later\n",
    "                future_entries[column] = [data[column].tail(1).values[0]] * future_periods\n",
    "\n",
    "# Check if 'Dividends' exists in the original data and add if it does\n",
    "    if 'Dividends' in data.columns:\n",
    "        future_entries['Dividends'] = [data['Dividends'].tail(1).values[0]] * future_periods\n",
    "\n",
    "# Check if 'Stock Splits' exists in the original data and add if it does\n",
    "    if 'Stock Splits' in data.columns:\n",
    "        future_entries['Stock Splits'] = [data['Stock Splits'].tail(1).values[0]] * future_periods\n",
    "# Convert future_entries to DataFrame\n",
    "    future_entries_df = pd.DataFrame(future_entries)\n",
    "\n",
    "# Step 4: Add future entries to the cloned DataFrame\n",
    "    future_df = pd.concat([future_df, future_entries_df], ignore_index=True)\n",
    "\n",
    "# Ensure 'y' column is present for predictions\n",
    "    if 'y' not in future_df.columns:\n",
    "        future_df['y'] = np.nan\n",
    "\n",
    "    forecast = model.predict(future_df)\n",
    "    return forecast\n",
    "\n",
    "\n",
    "#This is the part that moves the data forward for next prediction\n",
    "def extendData(data2extend,forecastedOpen, forecastedClose, forecastedVolume, forecastedEPS):\n",
    "    data2extend.loc[len(data2extend)] = [None] * len(data2extend.columns)\n",
    "    \n",
    "\n",
    "    if 'Open' not in data2extend.columns:\n",
    "        data2extend['y'] = data2extend['y'].where(forecastedOpen['yhat1'].isna(), forecastedOpen['yhat1'])\n",
    "        data2extend['Close'] = data2extend['Close'].where(forecastedClose['yhat1'].isna(), forecastedClose['yhat1'])\n",
    "        data2extend['Volume'] = data2extend['Volume'].where(forecastedVolume['yhat1'].isna(), forecastedVolume['yhat1'])\n",
    "        data2extend['Reported EPS'] = data2extend['Reported EPS'].where(forecastedEPS['yhat1'].isna(), forecastedEPS['yhat1'])\n",
    "        data2extend.loc[data2extend.index[-1], 'Surprise(%)'] = ((data2extend['Reported EPS'].iloc[-1] - data2extend['EPS Estimate'].iloc[-1]) / data2extend['EPS Estimate'].iloc[-1])\n",
    "\n",
    "    if 'Close' not in data2extend.columns:\n",
    "        data2extend['Open'] = data2extend['Open'].where(forecastedOpen['yhat1'].isna(), forecastedOpen['yhat1'])\n",
    "        data2extend['y'] = data2extend['y'].where(forecastedClose['yhat1'].isna(), forecastedClose['yhat1'])\n",
    "        data2extend['Volume'] = data2extend['Volume'].where(forecastedVolume['yhat1'].isna(), forecastedVolume['yhat1'])\n",
    "        data2extend['Reported EPS'] = data2extend['Reported EPS'].where(forecastedEPS['yhat1'].isna(), forecastedEPS['yhat1'])\n",
    "        data2extend.loc[data2extend.index[-1], 'Surprise(%)'] = ((data2extend['Reported EPS'].iloc[-1] - data2extend['EPS Estimate'].iloc[-1]) / data2extend['EPS Estimate'].iloc[-1])\n",
    "\n",
    "    if 'Volume' not in data2extend.columns:\n",
    "        data2extend['Open'] = data2extend['Open'].where(forecastedOpen['yhat1'].isna(), forecastedOpen['yhat1'])\n",
    "        data2extend['Close'] = data2extend['Close'].where(forecastedClose['yhat1'].isna(), forecastedClose['yhat1'])\n",
    "        data2extend['y'] = data2extend['y'].where(forecastedVolume['yhat1'].isna(), forecastedVolume['yhat1'])\n",
    "        data2extend['Reported EPS'] = data2extend['Reported EPS'].where(forecastedEPS['yhat1'].isna(), forecastedEPS['yhat1'])\n",
    "        data2extend.loc[data2extend.index[-1], 'Surprise(%)'] = ((data2extend['Reported EPS'].iloc[-1] - data2extend['EPS Estimate'].iloc[-1]) / data2extend['EPS Estimate'].iloc[-1])\n",
    "\n",
    "    if 'Reported EPS' not in data2extend.columns:\n",
    "        data2extend['Open'] = data2extend['Open'].where(forecastedOpen['yhat1'].isna(), forecastedOpen['yhat1'])\n",
    "        data2extend['Close'] = data2extend['Close'].where(forecastedClose['yhat1'].isna(), forecastedClose['yhat1'])\n",
    "        data2extend['Volume'] = data2extend['Volume'].where(forecastedVolume['yhat1'].isna(), forecastedVolume['yhat1'])\n",
    "        data2extend['y'] = data2extend['y'].where(forecastedEPS['yhat1'].isna(), forecastedEPS['yhat1'])\n",
    "        data2extend.loc[data2extend.index[-1], 'Surprise(%)'] = ((data2extend['y'].iloc[-1] - data2extend['EPS Estimate'].iloc[-1]) / data2extend['EPS Estimate'].iloc[-1])\n",
    "\n",
    "    \n",
    "    last_date = data2extend['ds'].max()\n",
    "    future_periods = 1  # Specify how many future days you want\n",
    "    future_dates = pd.date_range(start=last_date + BDay(1), periods=future_periods, freq=BDay())\n",
    "    data2extend['ds'].tail(1).values[0] = future_dates[0]\n",
    "    data2extend.ffill(inplace=True) #Front fill needs to be add end, or we get drop columns error\n",
    "\n",
    "\n",
    "    \n",
    "    return data2extend\n",
    "#####\n",
    "\n",
    "future_dfOpen = Openmodeldata.copy()\n",
    "future_dfClose = Closemodeldata.copy()\n",
    "future_dfVolume = Volumedata.copy()\n",
    "future_dfReportedeps = Reportedepsdata.copy()\n",
    "\n",
    "forecastOpen = predictFuture(future_dfOpen, Openmodel)\n",
    "forecastClose = predictFuture(future_dfClose, Closemodel)\n",
    "forecastVolume = predictFuture(future_dfVolume, Volumemodel)\n",
    "forecastReportedeps = predictFuture(future_dfReportedeps, Reportedepsmodel)\n",
    "periods = 7\n",
    "count = 0\n",
    "while count < periods:\n",
    "    future_dfOpen = extendData(future_dfOpen, forecastOpen,forecastClose,forecastVolume,forecastReportedeps)\n",
    "    future_dfClose = extendData(future_dfClose, forecastOpen,forecastClose,forecastVolume,forecastReportedeps)\n",
    "    future_dfVolume = extendData(future_dfVolume, forecastOpen,forecastClose,forecastVolume,forecastReportedeps)\n",
    "    future_dfReportedeps = extendData(future_dfReportedeps, forecastOpen,forecastClose,forecastVolume,forecastReportedeps)\n",
    "    forecastOpen = predictFuture(future_dfOpen, Openmodel)\n",
    "    forecastClose = predictFuture(future_dfClose, Closemodel)\n",
    "    forecastVolume = predictFuture(future_dfVolume, Volumemodel)\n",
    "    forecastReportedeps = predictFuture(future_dfReportedeps, Reportedepsmodel)\n",
    "    count += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecastOpen[['ds','y','yhat1','lagged_regressor_Surprise(%)1', 'lagged_regressor_Reported EPS1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in forecastOpen.columns:\n",
    "    if 'EPS' in i:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forecast = Closemodel.predict(future_dfClose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forecast[['ds','y','yhat1']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prophetGPU2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
